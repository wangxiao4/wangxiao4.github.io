<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on Xiao4 800K</title>
    <link>http://www.wangxiao4.xyz/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on Xiao4 800K</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-Hans</language>
    <copyright>版权所有©2019–2021，Xiao4 800K；保留所有权利。</copyright>
    <lastBuildDate>Sun, 07 Feb 2021 09:46:11 +0800</lastBuildDate><atom:link href="http://www.wangxiao4.xyz/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>抓百度下拉框内容</title>
      <link>http://www.wangxiao4.xyz/blog/bddorpdown/</link>
      <pubDate>Sun, 07 Feb 2021 09:46:11 +0800</pubDate>
      
      <guid>http://www.wangxiao4.xyz/blog/bddorpdown/</guid>
      <description>百度？？？你要干嘛 朋友总是有很多奇怪的需求，这次他想要百度搜索智能提示的内容，就是这个
至于他要干吗用，这个并没有太多去问，只是一时兴起，决定白嫖百度一下 开门见山的说，白嫖不好，如果大家有能力还是去购买相应的接口，等我有钱了就买百度云在抓数据😊
如何做 朋友给了我一个非常机智的方法
0. 打开电脑，打开浏览器 1. 让程序模拟用户操作，在浏览器的输入框中输入预存的词组 2. 截取当前浏览器的显示截图 3. 程序识别图中内容 4. 将内容保存到本地  至此完成一次操作，后续就是无限循环的抓取动作
我的想法就比较简单粗暴了，因为数据不可能缓存在本地，所以每次输入内容，百度一定会回服务器抓取的， 于是我打开浏览器，控制台监控浏览器的请求，发现了一些频率较高的重复Get请求，打开一看，哎嘿……这是啥
整理下思路 众所周知，百度引擎会爬各种网站的数据，所以爬虫对他们来说那是相当的熟悉，所以肯定会限制我们爬取他们 那么首要任务就是要测试这些Get请求是否有限制，最简单的方法直接PostMan测试一下，修改各种参数，头信息， 如果能够正常返回，剩下就是我们如何处理这些数据了
杨帆启航 0. 查找接口 1. 首先PostMan测试接口情况 2. 整理数据 3. 编写脚本 4. 运行测试 5. 简单压测   结果五根线程110词在2秒完成 ……666 最后附赠测试代码地址 csdn：https://download.csdn.net/download/at555444/15118651  </description>
    </item>
    
    <item>
      <title>谷歌浏览器代理如何玩</title>
      <link>http://www.wangxiao4.xyz/blog/chromeproxy/</link>
      <pubDate>Mon, 01 Feb 2021 11:14:08 +0800</pubDate>
      
      <guid>http://www.wangxiao4.xyz/blog/chromeproxy/</guid>
      <description>谷歌浏览器代理如何玩 在我们玩爬虫的时候，通常需要访问大量的站点，而对百度或者谷歌这类搜索引擎对爬虫通常都有着很强的限制 比如当你的爬虫大量抓取百度的时候，百度会给你提示输入验证码等措施避免你频繁的访问， 当然想要解决这种问题，有很多种方法，比如使用代理，对目标站点隐藏自己的信息， 或者写对应的验证代码，通过验证限制
当然我认为使用代理是比较方便，且直接的方法 接下来就说说，我遇到的情况 首先先说一下这个小工具的目标，通过批量输入要查询的站点域名，查询域名在不同搜索引擎中的收录情况 包含：百度，谷歌，搜狗，360搜索，Archive 等
在经过测后发现，通常谷歌浏览器在同时开启50个搜索页面后，会有明显的卡顿，可能和我机器有一定的关系 但是无关紧要，关键的问题是，当同时开启50+的抓取行为后，上述的搜索引擎会弹出验证，这无疑增加了 操作人员的工作量，所以这就是我们现在要处理的问题之一，增加代理访问，绕过验证
在哪里增加，如何增加？  首先增加代理要在哪里增加哪？ 最开始我只发现了Archive在快速访问的时候会出现断开的问题，所以我想给这个站点增加一层代理， 也就是说针对这个站点有效的代理，市面上的各种代理，通常都是针对PC网卡端或浏览器进行监听并转发代理模式去操作 这明显和我的想要的不太一样，所以我准备镜像这个站点，快速的写了一些代码将它镜像做好，在镜像处理的同时增加一层代理 成功绕过限制后进行批量测试……百度首先发送了一个友好的验证码  我意识到可能需要在整体增加代理，在哪里增加比较合适那？无疑如果我在监听网卡可能会更麻烦，索性直接在Chrome浏览器增加算了 查看ChromeAPI 发现了有用的接口 1. &amp;ndash;proxy-server 启动命令中增加一个代理地址，但是由于我的访问量很大所以使用一个或多个并不起作用 2. pac文件	通过增加pac文件动态修改代理IP 这就很有意思了
 如何增加那？ 由于Chrome直接增加pac文件很麻烦，所以使用SwitchyOmega插件  安装SwitchyOmega插件，并打开管理面板 新建模式 选择PAC情景模式 创建模式，并选中 增加PAC脚本代码，据说是JS代码，并包含FindProxyForURL(url, host) 方法即可 测试一下 这时已经成功的设置了代理 你可以在脚本中写一些随机函数，让每一次请求都使用不同代理，也可以在PAC网址处增加动态获取接口 而我则使用了后者，因为这样使用起来更加方便，在工具中增加一个API用于提供PAC脚本，并在后台实时刷新可用的代理地址， SwitchyOmega插件中设置15分钟刷新一次PAC脚本，这样在基本上完成了代理的动态设置    不足之处 SwitchyOmega插件 最低刷新频率是15分钟，这里在某些层面上限制了代理的更换速率，可以通过修改插件完成自己想要的效果 不过对于我来说 还算够用
有些问题只有你自己经历了才会记忆深刻，多动手，多实验，总会找到办法 希望能对你有所帮助</description>
    </item>
    
  </channel>
</rss>
